{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkskTZ7J8--_",
        "outputId": "5250b412-c8de-494c-9cdd-9e92155536d7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnp-oO5g9Ws8",
        "outputId": "b5061f48-3cc6-4973-93d9-60d5ba6a5014"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install CRPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taR19He7ZjLW",
        "outputId": "ca88e0d9-3d14-494a-c38d-87fee02de8b6"
      },
      "outputs": [],
      "source": [
        " \n",
        "from pathlib import Path\n",
        "import sys\n",
        "!sudo apt-get install unzip\n",
        "base = Path('/content/drive/MyDrive/ML/2023/conv_strat_dataset')\n",
        "sys.path.append(str(base))\n",
        "\n",
        "zip_path = base/\"lwe_dataset_010322.zip\"\n",
        "\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "\n",
        "!unzip -q lwe_dataset_010322.zip -d \"/content\"\n",
        "\n",
        "!rm lwe_dataset_010322.zip\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe8ibXtZ87sM",
        "outputId": "a0cfca16-2f27-4aa0-a8c5-95138bed47a2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# import wandb\n",
        "# import CRPS.CRPS as pscore\n",
        "\n",
        "\n",
        "from prec_dataset import RadarPrecipitationSequence\n",
        "from radar_transforms import radar_transform\n",
        "from plotting_funcs import show_sequence\n",
        "from radar_transforms import radar_transform, reverse_transform\n",
        "from fdp import fdp_sample, get_named_beta_schedule\n",
        "from unet import UNet\n",
        "from loss import loss_fn\n",
        "from sampler import sample_plot_image\n",
        "from helper_module import get_random_test_seq, get_CRPS_sequence\n",
        "from calc_metrics import get_CRPS\n",
        "\n",
        "config = dict(\n",
        "    img_out_size=64,\n",
        "    rgb_grayscale=1,\n",
        "    sequence_length=5,\n",
        "    max_prec_val=3.4199221045419974,\n",
        "    prediction_time_step_ahead=1,\n",
        "    frames_to_predict=1,\n",
        "    num_cond_frames=4,\n",
        "    epochs=150,\n",
        "    batch_size=24,\n",
        "    lr=0.001,\n",
        "    T=1000,\n",
        "    schedule=\"linear\",\n",
        "    root_dir=r\"/content/lwe_dataset_010322\",\n",
        "    validate_on_convective = False,\n",
        "    plot_folder = \"/content/drive/MyDrive/ML/results/plots/0503_concat_t_1000\"\n",
        ")\n",
        "\n",
        "plot_folder = config[\"plot_folder\"]\n",
        "with open(f'{plot_folder}/config.json', 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "betas = get_named_beta_schedule(\n",
        "    schedule_name=config[\"schedule\"], num_diffusion_timesteps=config[\"T\"]\n",
        ")\n",
        "T = config[\"T\"]\n",
        "if config[\"schedule\"] == \"linear\":\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UNet(\n",
        "    rgb_grayscale=config[\"rgb_grayscale\"], num_cond_frames=config[\"num_cond_frames\"], device=device\n",
        ")\n",
        "model.to(device)\n",
        "print(\"Num params in Unet: \", sum(p.numel() for p in model.parameters()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgmVVa4-87sQ"
      },
      "source": [
        "### Define datasets and dataloader: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS2gqHvq87sR"
      },
      "outputs": [],
      "source": [
        "dataset_radar_sequence = RadarPrecipitationSequence(\n",
        "    root_dir=config[\"root_dir\"],\n",
        "    transform=radar_transform(max_prec_val=config[\"max_prec_val\"]),\n",
        "    num_cond_frames=config[\"num_cond_frames\"],\n",
        "    frames_to_predict=config[\"frames_to_predict\"],\n",
        "    img_out_size=config[\"img_out_size\"],\n",
        "    prediction_time_step_ahead=config[\"prediction_time_step_ahead\"],\n",
        "    train_test_val = \"train\"\n",
        ")\n",
        "dataset_radar_sequence_val = RadarPrecipitationSequence(\n",
        "    root_dir=config[\"root_dir\"],\n",
        "    transform=radar_transform(max_prec_val=config[\"max_prec_val\"]),\n",
        "    num_cond_frames=config[\"num_cond_frames\"],\n",
        "    frames_to_predict=config[\"frames_to_predict\"],\n",
        "    img_out_size=config[\"img_out_size\"],\n",
        "    prediction_time_step_ahead=config[\"prediction_time_step_ahead\"],\n",
        "    train_test_val = \"val\"\n",
        ")\n",
        "\n",
        "dataset_radar_sequence_val_CRPS = RadarPrecipitationSequence(\n",
        "    root_dir=config[\"root_dir\"],\n",
        "    transform=radar_transform(max_prec_val=config[\"max_prec_val\"]),\n",
        "    num_cond_frames=config[\"num_cond_frames\"],\n",
        "    frames_to_predict=config[\"frames_to_predict\"],\n",
        "    img_out_size=config[\"img_out_size\"],\n",
        "    prediction_time_step_ahead=config[\"prediction_time_step_ahead\"],\n",
        "    train_test_val = \"val\",\n",
        "    center_crop = True\n",
        ")\n",
        "\n",
        "dataset_radar_sequence_test = RadarPrecipitationSequence(\n",
        "    root_dir=config[\"root_dir\"],\n",
        "    transform=radar_transform(max_prec_val=config[\"max_prec_val\"]),\n",
        "    num_cond_frames=config[\"num_cond_frames\"],\n",
        "    frames_to_predict=config[\"frames_to_predict\"],\n",
        "    img_out_size=config[\"img_out_size\"],\n",
        "    prediction_time_step_ahead=config[\"prediction_time_step_ahead\"],\n",
        "    train_test_val = \"test\"\n",
        "\n",
        ")\n",
        "\n",
        "# dataset_radar_sequence_test = RadarPrecipitationSequence(root_dir=\"dataset_1000_five_seq\", transform= radar_transform(IMG_SIZE=img_out_size), output_img_size=img_out_size, train=False)\n",
        "dataloader = DataLoader(\n",
        "    dataset_radar_sequence,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    dataset_radar_sequence_val,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "validation_dataloader_CRPS = DataLoader(\n",
        "    dataset_radar_sequence_val_CRPS,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEFxphfH87sS"
      },
      "source": [
        "### Random samples from dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zen6ObkK87sS"
      },
      "outputs": [],
      "source": [
        "# for i in range(3):\n",
        "#     idx = np.random.randint(low=0, high=250)\n",
        "#     test_sample = dataset_radar_sequence_test.__getitem__(idx)\n",
        "\n",
        "#     show_sequence(test_sample, config[\"sequence_length\"], pred_ahead= config[\"prediction_time_step_ahead\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfMhUHcH87sT"
      },
      "source": [
        "### Simulate forward diffusion process: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX-Ng5OH87sT"
      },
      "outputs": [],
      "source": [
        "# simulate_fdp(forward_diffusion_sample=forward_diffusion_sample, dataloader=dataloader,sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod, sqrt_alphas_cumprod=sqrt_alphas_cumprod, T=config[\"T\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJWENxEg87sU"
      },
      "source": [
        "### Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XdO3cHtg87sU",
        "outputId": "910f0a93-1dd4-4e02-85e1-d5a0e10b3344"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "train_loss_list = []\n",
        "validation_loss_list = []\n",
        "avg_4_crps_mean = []\n",
        "avg_16_crps_mean = []\n",
        "max_4_crps_mean = []\n",
        "max_16_crps_mean = []\n",
        "\n",
        "avg_4_crps_std = []\n",
        "avg_16_crps_std = []\n",
        "max_4_crps_std = []\n",
        "max_16_crps_std = []\n",
        "\n",
        "\n",
        "crps_idx_list = [57,\n",
        " 460,\n",
        " 63,\n",
        " 203,\n",
        " 357,\n",
        " 164,\n",
        " 327,\n",
        " 470,\n",
        " 260,\n",
        " 161,\n",
        " 140,\n",
        " 404,\n",
        " 379,\n",
        " 451,\n",
        " 289,\n",
        " 79,\n",
        " 141,\n",
        " 76,\n",
        " 42,\n",
        " 47]\n",
        "\n",
        "\n",
        "\n",
        "best_val_score = 1\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    avg_epoch_train_loss = 0 \n",
        "    \n",
        "    for step, batch in enumerate(tqdm(dataloader)):\n",
        "        \n",
        "        batch_lwe = batch[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        t = torch.randint(0, config[\"T\"], (config[\"batch_size\"],), device=device).long()\n",
        "\n",
        "        conditional_imgs = batch_lwe[:, 0:-1, :, :]\n",
        "\n",
        "        imgs_to_model_training = batch_lwe[:, -1, :, :]\n",
        "        imgs_to_model_training = imgs_to_model_training[:, None, :, :]\n",
        "        # print(f\"imgs_to_model_training.shape = {imgs_to_model_training.shape}\")\n",
        "        # print(f\"conditional_imgs.shape = {conditional_imgs.shape}\")\n",
        "\n",
        "        conditional_imgs = conditional_imgs.to(device)\n",
        "        loss = loss_fn(\n",
        "            model=model,\n",
        "            x=imgs_to_model_training,\n",
        "            t=t,\n",
        "            device=device,\n",
        "            sqrt_one_minus_alphas_cumprod=sqrt_one_minus_alphas_cumprod,\n",
        "            sqrt_alphas_cumprod=sqrt_alphas_cumprod,\n",
        "            condition=conditional_imgs,\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_epoch_train_loss += loss.item()\n",
        "        \n",
        "    avg_epoch_train_loss = avg_epoch_train_loss/(step+1)\n",
        "    \n",
        "    #get validation loss for same epoch\n",
        "    avg_epoch_val_loss = 0 \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for vstep, vbatch in enumerate(tqdm(validation_dataloader)):\n",
        "            batch_lwe = vbatch[0]\n",
        "\n",
        "            t = torch.randint(0, config[\"T\"], (config[\"batch_size\"],), device=device).long()\n",
        "\n",
        "            conditional_imgs = batch_lwe[:, 0:-1, :, :]\n",
        "\n",
        "            imgs_to_model_training = batch_lwe[:, -1, :, :]\n",
        "            imgs_to_model_training = imgs_to_model_training[:, None, :, :]\n",
        "            # print(f\"imgs_to_model_training.shape = {imgs_to_model_training.shape}\")\n",
        "            # print(f\"conditional_imgs.shape = {conditional_imgs.shape}\")\n",
        "\n",
        "            conditional_imgs = conditional_imgs.to(device)\n",
        "            loss =  loss_fn(\n",
        "                model=model,\n",
        "                x=imgs_to_model_training,\n",
        "                t=t,\n",
        "                device=device,\n",
        "                sqrt_one_minus_alphas_cumprod=sqrt_one_minus_alphas_cumprod,\n",
        "                sqrt_alphas_cumprod=sqrt_alphas_cumprod,\n",
        "                condition=conditional_imgs,\n",
        "            )\n",
        "            avg_epoch_val_loss += loss.item()\n",
        "        \n",
        "        avg_epoch_val_loss = avg_epoch_val_loss/(vstep+1)\n",
        "        if avg_epoch_val_loss < best_val_score:\n",
        "            best_val_score = avg_epoch_val_loss\n",
        "            today = date.today()\n",
        "            torch.save(model.state_dict(), f\"{plot_folder}/{today}_epoch_{epoch}\")\n",
        "\n",
        "        print(f\"Epoch {epoch} | Avg Train Loss: {avg_epoch_train_loss}, Avg Validation Loss: {avg_epoch_val_loss} \")\n",
        "        train_loss_list.append(avg_epoch_train_loss)\n",
        "        validation_loss_list.append(avg_epoch_val_loss)\n",
        "\n",
        "        \n",
        "    # if convective_crps:\n",
        "        # get_convective_test_seq()\n",
        "\n",
        "        #get CRPS \n",
        " \n",
        "            #set model back to training mode: \n",
        "    model.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9DZo1wIYMSd"
      },
      "outputs": [],
      "source": [
        "val_loss_arr = np.asarray(validation_loss_list)\n",
        "train_loss_arr = np.asarray(train_loss_list)\n",
        "\n",
        "np.savetxt(f\"{plot_folder}/val_loss_arr.csv\", val_loss_arr, delimiter=\",\")\n",
        "np.savetxt(f\"{plot_folder}/train_loss_arr.csv\", train_loss_arr, delimiter=\",\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ML_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "54d325ac27653bbd16399bc9890710378eb69529f792fa6d179297359bb410f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
